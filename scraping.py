from selenium import webdriverfrom selenium.webdriver import Safari, Chromefrom selenium.webdriver.common.by import Byfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver.common.action_chains import ActionChainsimport reimport timeimport listingwindowfrom bs4 import BeautifulSoupfrom urllib.parse import urljoindriver = Chrome()job_titles = []job_links = []# ul, ol, div with role of list# open is a common word associated with buttons that lead to opportunities# can be button or a elements# ALGORITHM:# 1. search for an unordered list (and copy all a elements)# 2. if there is no unordered list, scan all the existing a elements in the document# 2. search for a button that contains the word "open", and if found click it# 3. scan all 'a' elements you can find that match the keyword if there is a button that existsdef search_links(list_of_links,keyword):    global driver        for link in list_of_links:                search_link(str(link.get() + " " + keyword),driver,keyword)        time.sleep(2.5)    listingwindow.generate_pages(job_titles,job_links)def open_link(link,job_name):        global driver        try:                driver.get(link)            except:                return    # searching of an individual linkdef search_link(phrase,driver,keyword):    driver.get('https://www.bing.com')    global job_titles, job_links, is_job_site    search_text_area = driver.find_element(By.ID,'sb_form_q')    search_text_area.clear()    time.sleep(1)    search_text_area.send_keys(phrase)    time.sleep(1)    search_text_area.send_keys(Keys.ENTER)    time.sleep(1)        page_source = driver.page_source    page_url = driver.current_url        soup = BeautifulSoup(page_source,'html.parser')    links_all = soup.find_all('a',string=lambda text: text and keyword.lower() in text.lower())        curr_link = 0        job_titles.append("LISTINGS FOR " + phrase.upper())    job_links.append("Not a link")        while curr_link < len(links_all):                try:                        driver.get(links_all[curr_link]['href'])                        time.sleep(5)                        soup = BeautifulSoup(driver.page_source,'html.parser')                                uls = soup.find_all('ul')                        if len(uls) > 0:                                for ul in uls:                                        links = ul.find_all('a',string=lambda text: text and keyword.lower() in text.lower())                                        for link in links:                                                job_titles.append(link.text)                        job_links.append(urljoin(driver.current_url,link['href']))                        else:                                links = soup.find_all('a',string=lambda text: text and keyword.lower() in text.lower())                                for link in links:                                        job_titles.append(link.text)                    job_links.append(urljoin(driver.current_url,link['href']))                                button = soup.find('a',string=lambda text: text and ['open'] in text.lower())                        if button:                                driver.get(button['href'])                                time.sleep(5)                                uls = soup.find_all('ul')                                if len(uls) > 0:                                        for ul in uls:                                                links = ul.find_all('a',string=lambda text: text and keyword.lower() in text.lower())                                                for link in links:                                                        job_titles.append(link.text)                            job_links.append(urljoin(driver.current_url,link['href']))                                            else:                                        links = soup.find_all('a',string=lambda text: text and keyword.lower() in text.lower())                                        for link in links:                                                job_titles.append(link.text)                        job_links.append(urljoin(driver.current_url,link['href']))        except:                        print("Failed link")                        curr_link += 1        driver.get(page_url)    